---
title: "Cyclistic Data Analysis"
author: "Anne"
date: "2024-02-07"
output: html_document
---

## Loading required packages
```{r}
library(tidyverse)
library(janitor)
```
## Importing the bike-share data
All the csv files for 2023 are imported and combined together to create the data frame bike_share_2023.
list.files produces a character vector of all the relative file paths in the folder *bike_share_csv_files*.
lapply applies the function read_csv to the file paths in the vector
bind_rows combines all the rows into one data frame.

```{r}
bike_share_2023 <- list.files(path = "bike_share_csv_files", pattern = "*.csv", full.names = TRUE) %>% 
  lapply(read_csv) %>% 
  bind_rows()
```
Now look at the data

```{r}
bike_share_2023
```

```{r}
View(bike_share_2023)
```

There are rows with *na* values in some columns. Check which columns these are and how many rows are affected.
```{r}
for (i in 1:13) {
  print(paste("Column ", i, ": ", sum(is.na(bike_share_2023[,i]))))
}
```

Remove all rows with *na* anywhere since the data in those rows is probably not reliable.
```{r}
bike_share_2023_cleaned <- bike_share_2023 %>% 
  drop_na()
```

Check that all the *na*s have been removed and look at what is left.

```{r}
for (i in 1:13) {
  print(paste("Column ", i, ": ", sum(is.na(bike_share_2023_cleaned[,i]))))
}
glimpse(bike_share_2023_cleaned)
```
There are 4,331,707 rows left out of the original 5,719,877 so about one quarter of the original data has now been deleted.

Next, the data is checked to make sure there are no duplicates. This is done by checking the *ride_id* column, which should be unique.

```{r}
bike_share_2023_cleaned %>% 
  get_dupes(ride_id)

```


Sort the data in date order (order by started_at).
```{r}
bike_share_2023_cleaned <- bike_share_2023_cleaned %>%
  arrange(started_at)
```


```{r}
head(bike_share_2023_cleaned)
```
Check that the only values in the member_casual column are 'member' and 'casual'.
```{r}
bike_share_2023_cleaned %>% 
  filter((member_casual != "member") & (member_casual != "casual"))
```

Look at a summary of the data to check for potentially problematic values.
```{r}
summary(bike_share_2023_cleaned)
```
There is a Min. of 0.00 for *end_lat* and a *Max* of 0.00 for the *end_lng*. This seems unlikely. Sort the data to look at these rows.
```{r}
bike_share_2023_cleaned %>% 
  arrange(end_lat)
```
There are three rows with end_lat and end_lng equal to zero. The first two seem to be some sort of test, with the ride time at less than five minutes. These rows can be removed. The third row has an end_station_name and end_station_id so we should be able to replace the end_lat and end_lng with the correct values.
Filter the data to find the correct latitude and longitude for station 653B.
```{r}
bike_share_2023_cleaned %>% 
  filter(end_station_id == "653B")
```
The end_lat is 41.78000 and the end_lng is -87.59000. Replace the zero values with these for ride_id 43107577DF9B498D, but first check that the end_lat and and end_lng are all the same for this station (excluding the single row for which they are zero).

```{r}
df <- bike_share_2023_cleaned %>% 
  filter((end_station_id == "653B") & (end_lat != 0)) %>% 
  summarise(max_end_lat = max(end_lat), min_end_lat = min(end_lat),
            max_end_lng = max(end_lng), min_end_lng = min(end_lng))
```
```{r}
df
```


A quick inspection of the data shows that two values are used for end_lat and end_long, but this is just due to rounding. We will set the values here to the less rounded values, given here as max_end_lat and max_end_lng.

```{r}
bike_share_2023_cleaned$end_lat[bike_share_2023_cleaned$ride_id == "43107577DF9B498D"] <- df$max_end_lat[1]
bike_share_2023_cleaned$end_lng[bike_share_2023_cleaned$ride_id == "43107577DF9B498D"] <- df$max_end_lng[1]
```

Now remove the two remaining rows with zero for end_lat and end_lng.
```{r}
bike_share_2023_cleaned <- bike_share_2023_cleaned %>% 
  filter(end_lat != 0)
```

Need to check that the ended_at times are all later than the corresponding started_at times. This should only be possible on 2023-11-05 when the daylight saving time ended and the clocks went backward. 

```{r}
bike_share_2023_cleaned %>% 
  filter(ended_at < started_at)
```
The data show 66 rows have ended_at times earlier than the started_at times, a significant number of which are related to daylight saving time. Since these are only a few rows given the size of the data set, I will delete them.
```{r}
bike_share_2023_cleaned <- bike_share_2023_cleaned %>% 
  filter(ended_at >= started_at)
```

Inspect the rideable_type to see what values this takes, and how many occurrences of each.
```{r}
bike_share_2023_cleaned %>% 
  group_by(rideable_type) %>% 
  summarise(count = n())
```
Some internet research shows that classic bikes were called docked bikes before electric bikes were introduced to the scheme in July 2020 - the docked bikes in this data are classic bikes that have not been re-labelled. I shall change them to classic bikes.

```{r}
bike_share_2023_cleaned <-  bike_share_2023_cleaned %>% 
  mutate(rideable_type = ifelse(rideable_type == "docked_bike", "classic_bike", rideable_type))
```

And check
```{r}
bike_share_2023_cleaned %>% 
  group_by(rideable_type) %>% 
  summarise(count = n())
```

Now look at the summary again.
```{r}
summary(bike_share_2023_cleaned)
```

The data now looks good so save the new data frame.
```{r}
write_rds(bike_share_2023_cleaned, file = "bike_share_2023_cleaned.RDS")
write_csv(bike_share_2023_cleaned, "bike_share_2023_cleaned.csv")
```

## Transforming the data
It would be useful to know how long the rides were since this is something that might be different for casual riders and members, so I will create a ride_length column. The ride_length values are rounded to the nearest minute, since we don't need second level precision.
I also create a day_of_week column giving the day of the week the ride started.
It might also be useful to get the time of year - it might be that casual riders tend not to ride as much in the winter, so I add a month column.

```{r}
bike_share_2023_processed <-  bike_share_2023_cleaned %>% 
  add_column(ride_length = round(difftime(bike_share_2023_cleaned$ended_at, bike_share_2023_cleaned$started_at, units = "mins"))) %>% 
  add_column(day_of_week = wday(bike_share_2023_cleaned$started_at)) %>% 
  add_column(month = month(bike_share_2023_cleaned$started_at))
```

```{r}
glimpse(bike_share_2023_processed)
```
I would like to know the range of values in ride_length.
```{r}
bike_share_2023_processed %>% 
  summarise(mean_ride_length = mean(ride_length), median_ride_length = median(ride_length), min_ride_length = min(ride_length), max_ride_length = max(ride_length))
```
There is at least one ride of length zero and there is a ride of length 12136 minutes, that is over 8 days - this might be an outlier and may cause problems with the analysis.
Rides of less than one minute are unlikely to be genuine rides. Filter to look at the rides that are less than one minute.

```{r}
bike_share_2023_processed %>% 
  filter(ride_length < 1)
```

There are 55907 of these rows. I will delete them to avoid them skewing the final analysis.
```{r}
bike_share_2023_processed <- bike_share_2023_processed %>% 
  filter(ride_length > 1)
```

Now I will investigate the possible outlier. How many rides are more than 24 hours (1440 minutes)?
```{r}
bike_share_2023_processed %>% 
  arrange(-ride_length) %>% 
  filter(ride_length > 1440)
```
While there are not many rides longer than 24 hours, there are over 100 of them and they are likely to be real data so I will keep them.

```{r}
View(bike_share_2023_processed)
```
Save the processed data as bike_share_2023_processed.
```{r}
write_rds(bike_share_2023_processed, file = "bike_share_2023_processed.RDS")
write_csv(bike_share_2023_processed, "bike_share_2023_processed.csv")
```

## Analysis

```{r}
bike_share_2023_processed %>% 
  group_by(member_casual) %>% 
  summarise(min_duration = min(ride_length), max_duration = max(ride_length))
```


Bar chart of ride length, split into members and casual riders. Most of the rides are short so we will filter to just show those less than one hour.
```{r}
bike_share_2023_processed %>% 
  filter(ride_length < 60) %>% 
  ggplot(mapping = aes(x = as.numeric(ride_length), fill = member_casual)) +
  geom_histogram(binwidth = 5)

```
It might be better to show two histograms using facet_wrap.
```{r}
bike_share_2023_processed %>% 
  filter(ride_length < 60) %>% 
  ggplot(mapping = aes(x = as.numeric(ride_length), fill = member_casual)) +
  geom_histogram(binwidth = 5) +
  facet_wrap(~member_casual)
```
Now look at rides between 1 and 12 hours long.
```{r}
bike_share_2023_processed %>% 
  filter(ride_length > 60 & ride_length < 720) %>% 
  ggplot(mapping = aes(x = as.numeric(ride_length), fill = member_casual)) +
  geom_histogram(binwidth = 60) +
  facet_wrap(~member_casual)
```
And rides longer than 12 hours.
```{r}
bike_share_2023_processed %>% 
  filter(ride_length > 720) %>% 
  ggplot(mapping = aes(x = as.numeric(ride_length), fill = member_casual)) +
  geom_histogram(binwidth = 720) +
  facet_wrap(~member_casual)
```
From which we can see that the majority of rides are less than an hour for both casual riders and members but more casual riders are doing rides longer than an hour than members are.

Now consider what day of week do people start their ride - again split by casual riders and members.

```{r}
new_labels <- c("member" = "Cyclistic Members", "casual" = "Casual Riders")

p1 <- bike_share_2023_processed %>%
  ggplot(mapping = aes(x = day_of_week, fill = member_casual)) +
  geom_bar() +
  facet_wrap(~ member_casual, labeller = labeller(member_casual = new_labels)) +
  scale_fill_manual(values = c("#f28e2b","#4e79a7"))

```
There are more casual riders at weekends, whereas there are more members riding in the week.
```{r}

p1 + labs(x = "Day of week", y = "Number of trips (thousands)") +
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7), labels=c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat")) +
  scale_y_continuous(breaks = c(0, 100000, 200000, 300000, 400000), labels = c("0", "100", "200", "300", "400")) +
  theme(legend.position = "none",
        plot.title = element_text(face = "bold", hjust = 0.5),
        strip.background = element_rect(fill="white")) +
  labs(title = "Number of Trips",
       subtitle = "The number of trips made on each day of the week for casual riders and for Cyclistic members.") 
```



Now look at the month.
```{r}
bike_share_2023_processed %>%
  ggplot(mapping = aes(x = month, fill = member_casual)) +
  geom_bar() +
  facet_wrap(~member_casual)
```
Both members and casual riders ride less in the winter.
Now consider time of day.
```{r}
bike_share_2023_processed %>%
  ggplot(mapping = aes(x = hour(started_at), fill = member_casual)) +
  geom_bar() +
  facet_wrap(~member_casual)
```
There are clear peaks at the start and end of the working day for members but not for casual riders, which would suggest that fewer casual riders are using the bikes to commute to work (or they are working more variable hours).

Now look at locations.

To do this I will group by start_station_id to create a new data frame giving a count of the number of rides starting at each station.
```{r}
station_ride_nums <- bike_share_2023_processed %>%
  # Average the start_lat and start_lng because they differ in the later digits for the same station.
  select(start_station_id, start_station_name, start_lat, start_lng, member_casual) %>% 
  group_by(member_casual, start_station_id, start_station_name) %>%
  summarise(number_of_rides = n(), start_latitude = round(mean(start_lat), 3), start_longitude = round(mean(start_lng), 3), )
```
```{r}
station_ride_nums
```

Save this data frame to a csv file to look at in Tableau.
```{r}
write_csv(station_ride_nums, "station_ride_nums.csv")
write_rds(station_ride_nums, "station_ride_nums.RDS")
```

Now consider which are the top starting stations for casual users and for members and what proportion these make up of all rides.

```{r}
df <- station_ride_nums %>% 
  group_by(member_casual) %>% 
  reframe(start_station_id, start_station_name, total_rides = sum(number_of_rides), number_of_rides, percentage_of_rides = (number_of_rides / total_rides) * 100)
```


Now filter and sort for members.
```{r}
members <- df %>% 
  filter(member_casual == "member") %>% 
  arrange(-number_of_rides)
head(members, 30)
```
Filter and sort for casual riders.
```{r}
casual_riders <- df %>% 
  filter(member_casual == "casual") %>% 
  arrange(-number_of_rides)
head(casual_riders, 30)
```

```{r}
sum(head(casual_riders, 5)$percentage_of_rides)
```
```{r}
sum(head(casual_riders, 10)$percentage_of_rides)
```
The top ten stations for casual users account for 13% of rides, with the top five stations accounting for over 8% of rides.
```{r}
sum(head(casual_riders, 20)$percentage_of_rides)
```
And the top 20 stations account for 19%.

Now consider the top 20 stations for members.
```{r}
sum(head(members, 20)$percentage_of_rides)
```
Do the same analysis for end stations.
```{r}
end_ride_nums <- bike_share_2023_processed %>%
  # Average the end_lat and end_lng because they differ in the later digits for the same station.
  select(end_station_id, end_station_name, end_lat, end_lng, member_casual) %>% 
  group_by(member_casual, end_station_id, end_station_name) %>%
  summarise(number_of_rides = n(), end_latitude = round(mean(end_lat), 3), end_longitude = round(mean(end_lng), 3), )
```

```{r}
df <- end_ride_nums %>% 
  group_by(member_casual) %>% 
  reframe(end_station_id, end_station_name, total_rides = sum(number_of_rides), number_of_rides, percentage_of_rides = (number_of_rides / total_rides) * 100)
```

Now filter and sort for members.
```{r}
members <- df %>% 
  filter(member_casual == "member") %>% 
  arrange(-number_of_rides)
head(members, 30)
```
Filter and sort for casual riders.
```{r}
casual_riders <- df %>% 
  filter(member_casual == "casual") %>% 
  arrange(-number_of_rides)
head(casual_riders, 30)
```


Next I look at some summary statistics.

First mean of all trip ride_lengths for members and casuals.
```{r}
bike_share_2023_processed %>% 
  group_by(member_casual) %>% 
  summarise(mean_ride_length = mean(ride_length))
```


1. Mean of ride_length for each day of the week for members and casual_riders.

```{r}
days_of_rides <- bike_share_2023_processed %>% 
  group_by(day_of_week, member_casual) %>% 
  summarise(mean_ride_length = mean(ride_length), median_ride_length = median(ride_length))
head(days_of_rides,14)
```

```{r}
ggplot(days_of_rides, mapping = aes(x = day_of_week, y = mean_ride_length, fill = member_casual)) +
  geom_col() +
  facet_wrap(~member_casual)
```


```{r}
ggplot(days_of_rides, mapping = aes(x = day_of_week, y = as.numeric(median_ride_length), fill = member_casual)) +
  geom_col() +
  facet_wrap(~member_casual)
```
If I just consider the rides that are less than 24 hours, how dose this affect the means?
```{r}
days_of_rides <- bike_share_2023_processed %>%
  filter(ride_length < 1440) %>% 
  group_by(day_of_week, member_casual) %>% 
  summarise(mean_ride_length = mean(ride_length), median_ride_length = median(ride_length))
head(days_of_rides,14)
```
```{r}
ggplot(days_of_rides, mapping = aes(x = day_of_week, y = as.numeric(mean_ride_length), fill = member_casual)) +
  geom_col() +
  facet_wrap(~member_casual)
```


2. Mean of ride length by time of day (with rides over 24 hours excluded).
```{r}
times_of_rides <- bike_share_2023_processed %>%
  filter(ride_length < 1440) %>% 
  group_by(hour_of_day = hour(started_at), member_casual) %>% 
  summarise(mean_ride_length = mean(ride_length), median_ride_length = median(ride_length))
head(times_of_rides, 48)
```

```{r}
ggplot(times_of_rides, mapping = aes(x = hour_of_day, y = as.numeric(mean_ride_length), fill = member_casual)) +
  geom_col() +
  facet_wrap(~member_casual)
```

```{r}
ggplot(times_of_rides, mapping = aes(x = hour_of_day, y = as.numeric(median_ride_length), fill = member_casual)) +
  geom_col() +
  facet_wrap(~member_casual)
```

```{r}
bike_share_2023_processed %>% 
  filter(ride_length < 60) %>% 
  ggplot(mapping = aes(x = as.numeric(ride_length), fill = member_casual)) +
  geom_histogram(binwidth = 5) +
  facet_grid(member_casual~day_of_week)
```


```{r}
bike_share_2023_processed %>% 
  filter(60 < ride_length & ride_length < 720) %>% 
  ggplot(mapping = aes(x = as.numeric(ride_length), fill = member_casual)) +
  geom_histogram(binwidth = 60) +
  facet_grid(member_casual~day_of_week)
```


```{r}
bike_share_2023_processed %>% 
  filter(ride_length > 720) %>% 
  ggplot(mapping = aes(x = as.numeric(ride_length), fill = member_casual)) +
  geom_histogram(binwidth = 720) +
  facet_grid(member_casual~day_of_week)
```

Total number of trips.
```{r}
bike_share_2023_processed %>% 
  group_by(member_casual) %>% 
  summarise(total_num_trips = n())
```

Number of trips at weekends.
```{r}
bike_share_2023_processed %>% 
  filter(day_of_week == 1 | day_of_week == 7) %>% 
  group_by(member_casual) %>% 
  summarise(num_weekend_trips = n())
```

Sundays
```{r}
bike_share_2023_processed %>% 
  filter(day_of_week == 1) %>% 
  group_by(member_casual) %>% 
  summarise(num_weekend_trips = n())
```
Saturdays
Sundays
```{r}
bike_share_2023_processed %>% 
  filter(day_of_week == 7) %>% 
  group_by(member_casual) %>% 
  summarise(num_weekend_trips = n())
```

## Electric or classic bikes?

```{r}
bike_share_2023_processed <- read_rds("bike_share_2023_processed.RDS")
```


```{r}
bike_share_2023_processed %>% 
  group_by(member_casual, rideable_type) %>% 
  summarise(num_rides = n())
```

```{r}
ggplot(bike_share_2023_processed, mapping = aes(x = member_casual, fill = rideable_type)) +
  geom_bar(position = position_dodge())
```

Do these numbers change over the course of the year?
```{r}
ggplot(bike_share_2023_processed, mapping = aes(x = month, fill = rideable_type)) +
  geom_bar(position = position_dodge()) +
  facet_wrap(~ member_casual)
```
Or by day of week?
```{r}
ggplot(bike_share_2023_processed, mapping = aes(x = day_of_week, fill = rideable_type)) +
  geom_bar(position = position_dodge()) +
  facet_wrap(~ member_casual)
```




